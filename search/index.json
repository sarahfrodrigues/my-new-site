[{"content":"Background: My research topic Despite a significant decline in aviation fatal accident rates since the 1960s, recent fluctuations—such as the increase in total fatal accidents in 2024—highlight that aviation safety remains a critical concern. Effective risk management plays a foundational role in safety and relies on the timely and accurate identification of hazards.\nFigure1: Yearly total fatal accidents showing a general downward trend in fatal aviation accidents from 1990 to 2023. Data source:AIRBUS Accident and incident reports, such as those maintained by the National Transportation Safety Board (NTSB) and the Aviation Safety Reporting System (ASRS), serve as essential sources of information. These repositories contain rich qualitative narratives describing events and contributing factors. However, the unstructured nature of this textual data poses a major challenge for automated processing, limiting the scalability and consistency of insights derived from it.\nThesis Problem Statement The aviation industry faces challenges in quickly analyzing and responding to incidents despite safety advancements.\nProbabilistic risk assessment (PRA) is essential for hazard identification, but its automated large-scale implementation is deprived by the difficulty of processing unstructured data in accident reports.\nCurrent methods often fail to identify causal relationships from text, leading to incomplete, inaccurate, or overly simplistic risk models that are unable to reflect the complexity of aviation accidents.\nResearch Objectives Enhance keyword and feature extraction techniques from accident reports. Improve the identification of causal relationships between contributing factors. Construct causal models to gain a deeper understanding of accident mechanisms. Implement probabilistic reasoning through Bayesian networks. Methodology Data Collection: Unstructured narrative reports from the NTSB and ASRS databases are used as the primary input.\nText Preprocessing: This includes steps such as tokenization, lowercasing, punctuation removal, acronym expansion, and keyword detection.\nTopic Modeling: Techniques such as Latent Dirichlet Allocation (LDA) are applied to identify recurring themes and contextual clusters within the corpus.\nCausal Relationship Extraction: Rule based or deep learning Techniques.\nGraph Construction: Extracted causal pairs are assembled into a Directed Acyclic Graph (DAG), representing the directional relationships between contributing factors.\nBayesian Network Modeling: The DAG is then adapted into a probabilistic structure to perform reasoning under uncertainty and support safety decision-making.\nValidation and Optimization: The resulting models are evaluated for accuracy, completeness, and practical relevance to aviation safety experts.\nPresent support decision-making in aviation safety through enhanced risk analysis and hazard identification.\nLLM-Based Application Modules Large Language Models (LLMs) serve as the core reasoning components in two key tasks: acronym expansion and causal relation extraction. These tasks are detailed below. The code can be found at Github.\nAcronym Handling Aviation narratives are often filled with domain-specific acronyms (e.g., “ATC,” “VMC”) whose meanings vary depending on context. Our approach first identifies acronyms enclosed in parentheses using regular expressions and then applies heuristics to infer their likely full forms based on nearby capitalized words and initial-letter patterns. However, a single acronym may have multiple valid expansions. For instance, \u0026ldquo;ACM\u0026rdquo; can refer to Acceptable Means of Compliance or Airspace Control Measures depending on the operational context. To resolve this ambiguity, a local Large Language Model (LLM) is leverage, such as Mistral, for disambiguation.\nImplementation The system constructs a targeted prompt that presents the LLM with:\nthe surrounding sentence (context),\nthe ambiguous acronym (token), and\na list of possible expansions (candidates).\nThe LLM evaluates which candidate best fits the context and returns the most appropriate full form. This disambiguation step ensures that acronyms are expanded meaningfully and precisely, reducing the risk of incorrect data interpretation in downstream tasks such as causal relationship extraction or topic modeling.\nThe updated acronym map is continuously enriched by storing newly discovered expansions, enhancing the system\u0026rsquo;s adaptability and performance over time. This hybrid design—combining heuristic rules with LLM reasoning—enables accurate, context-sensitive acronym expansion at scale, which is essential for processing large volumes of aviation safety reports.\n1 2 3 4 5 6 7 8 9 [Input Text] ↓ [Regex Acronym Detection] ↓ [Heuristic Candidate Generation] ↓ [LLM Disambiguation] ↓ [Store in Acronym Map] Prompt Design\n1 2 3 4 5 6 7 8 prompt = ( f\u0026#34;You are given a sentence and a token. The token is ambiguous and could mean more than one thing.\\n\u0026#34; f\u0026#34;Your job is to choose the correct meaning based on the context.\\n\\n\u0026#34; f\u0026#34;Context sentence: \\\u0026#34;{context}\\\u0026#34;\\n\u0026#34; f\u0026#34;Token: {token}\\n\u0026#34; f\u0026#34;Choices: {\u0026#39;, \u0026#39;.join(candidates)}\\n\\n\u0026#34; f\u0026#34;Which meaning fits best in the sentence? Reply with ONLY the best choice, no explanation.\u0026#34; ) Performance evaluation To evaluate the accuracy of acronym extraction and disambiguation in aviation safety texts, we conducted a series of controlled test cases.\nAcronym Extraction Evaluation A total of 25 sentences were curated, each containing one aviation-specific acronym alongside its correct expansion. These were used to test the system’s ability to detect and map acronyms in structured narrative formats, such as those commonly found in aviation safety reports.\nExample Test Case:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Test cases for acronym extraction evaluation test_cases = [ { \u0026#34;text\u0026#34;: \u0026#34;The Aircraft Communications Addressing and Reporting System (ACARS) provides real-time communication between pilots and ground stations.\u0026#34;, \u0026#34;expected\u0026#34;: {\u0026#34;ACARS\u0026#34;: \u0026#34;Aircraft Communications Addressing and Reporting System\u0026#34;} }, { \u0026#34;text\u0026#34;: \u0026#34;The Airworthiness Directive (AD) requires immediate action on all Boeing 737 models.\u0026#34;, \u0026#34;expected\u0026#34;: {\u0026#34;AD\u0026#34;: \u0026#34;Airworthiness Directive\u0026#34;} }, { \u0026#34;text\u0026#34;: \u0026#34;The Emergency Response Plan (ERP) was activated following the runway incursion.\u0026#34;, \u0026#34;expected\u0026#34;: {\u0026#34;ERP\u0026#34;: \u0026#34;Emergency Response Plan\u0026#34;} }, { \u0026#34;text\u0026#34;: \u0026#34;Sensitive components are protected from Electrostatic Discharge (ESD) during assembly.\u0026#34;, \u0026#34;expected\u0026#34;: {\u0026#34;ESD\u0026#34;: \u0026#34;Electrostatic Discharge\u0026#34;} }, { \u0026#34;text\u0026#34;: \u0026#34;The Estimated Time of Arrival (ETA) for the flight is 14:35 local time.\u0026#34;, \u0026#34;expected\u0026#34;: {\u0026#34;ETA\u0026#34;: \u0026#34;Estimated Time of Arrival\u0026#34;} }, ] After running the full set of test cases, the system achieved the following performance metrics:\nPrecision: 0.85 indicates that the majority of the acronyms identified by the system were correctly matched with their true expansions. Recall: 0.77 shows that some acronyms present in the text were missed or improperly mapped. F1 Score: 0.81 reflects a balanced measure of overall performance. Acronym Disambiguation Evaluation The next stage tested the system’s ability to disambiguate acronyms that have more than one potential meaning, depending on context. This was achieved by:\nProviding a setup sentence in which the acronym is explicitly defined (e.g., “The Acceptable Means of Compliance (ACM) was published.”).\nFollowing this with a target sentence that reuses the acronym without expansion.\nAsking the system to infer the correct expansion using a local Large Language Model (LLM) and a set of candidate meanings.\nExample Test Case:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Test cases for acronym extraction evaluation disambiguation_tests = [ { \u0026#34;setup\u0026#34;: \u0026#34;The Acceptable Means of Compliance (ACM) was published.\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Pilots must comply with ACM before flight.\u0026#34;, # should expand to Acceptable Means of Compliance \u0026#34;expected\u0026#34;: \u0026#34;Acceptable Means of Compliance\u0026#34; }, { \u0026#34;setup\u0026#34;: \u0026#34;The Airspace Control Measures (ACM) were defined by the military.\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;ACM were enforced during the exercise.\u0026#34;, # should expand to Airspace Control Measures \u0026#34;expected\u0026#34;: \u0026#34;Airspace Control Measures\u0026#34; }, { \u0026#34;setup\u0026#34;: \u0026#34;The Emergency Response Plan (ERP) was activated.\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;After the crash, ERP was immediately launched.\u0026#34;, # should expand to Emergency Response Plan \u0026#34;expected\u0026#34;: \u0026#34;Emergency Response Plan\u0026#34; }, { \u0026#34;setup\u0026#34;: \u0026#34;The Estimated Time of Arrival (ETA) was confirmed.\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The ETA for Flight 8701 is now 15:45.\u0026#34;, # should expand to Estimated Time of Arrival \u0026#34;expected\u0026#34;: \u0026#34;Estimated Time of Arrival\u0026#34; }, ] Summary of Results:\nOut of 10 disambiguation test cases:\n✅ 9 acronyms were correctly disambiguated.\n❌ 1 acronyms were incorrectly expanded or left unresolved.\nExamples of correct matches include:\n“ACM were enforced during the exercise.” → Airspace Control Measures\n“ETA for Flight 8701 is now 15:45.” → Estimated Time of Arrival\n“FBI agents arrived on the scene.” → Federal Bureau of Investigation\nFailures included:\n“Technicians wore grounding straps to prevent ESD.” → Expected: Electrostatic Discharge, but the expansion was not correctly substituted. These results indicate strong performance by the disambiguation module, particularly in cases where the context provides clear semantic clues. However, further refinement is needed to handle subtle or technical uses where context is sparse or ambiguous.\nCausal Relation Extraction and DAG Construction To automatically identify causal relationships between safety-related concepts described in narrative reports and represent them as a structured causal graph. The goal is to process text (e.g., accident reports, news, academic papers) and build a graph of “X causes Y” relationships only if those concepts are in a predefined list of keywords.\nImplementation The core idea is to process individual sentences, detect whether multiple relevant keywords co-occur, and extract cause-effect relations between them using a local LLM.\nKeyword Filtering Each sentence is scanned to identify the presence of predefined domain-specific keywords (e.g., “weather,” “runway,” “delay,” “crew fatigue”).\nPrompt Design If two or more keywords appear, the sentence is submitted to the LLM with a strict instruction prompt such as:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \u0026#34;\u0026#34;\u0026#34;Analyze this sentence to identify ALL DIRECT causal relationships between these keywords: {\u0026#39;, \u0026#39;.join(sent_keywords)}. Follow these rules ABSOLUTELY: 1. Extract EVERY instance where Keyword X directly causes/influences Keyword Y 2. For chain relationships (A causes B causes C), extract BOTH A-\u0026gt;B AND B-\u0026gt;C 3. For multiple causes (A because B and C), extract BOTH B-\u0026gt;A AND C-\u0026gt;A 4. For multiple effects (B and C because A), extract BOTH A-\u0026gt;B AND A-\u0026gt;C 5. Output EXACTLY one \u0026#34;KeywordX -\u0026gt; KeywordY\u0026#34; per line, no additional text 6. Use ONLY exact keyword matches from the provided list 7. PRESERVE NEGATIONS (e.g., \u0026#34;not A causes B\u0026#34; yields A-\u0026gt;B) 8. IGNORE any relationships not explicitly stated in the sentence Examples: Sentence: \u0026#34;A because B and C. C caused E and D.\u0026#34; Keywords: [A, B, C, D, E] Output: B -\u0026gt; A C -\u0026gt; A C -\u0026gt; E C -\u0026gt; D Sentence: \u0026#34;Flooding occurred due to heavy rain and poor drainage, which then caused crop damage.\u0026#34; Keywords: [rain, drainage, flood, crop] Output: rain -\u0026gt; flood drainage -\u0026gt; flood flood -\u0026gt; crop --- Analyze THIS sentence: \u0026#34;{sent}\u0026#34; Output ONLY the causal pairs (no other text): \u0026#34;\u0026#34;\u0026#34; The model is set its temperature to low (0.2) to reduce randomness.\nThe model will return up to 300 tokens of plain causal statements (e.g., X -\u0026gt; Y). Takes documents (as lists of tokens) and joins them into sentences.\nOutput Processing The model output is cleaned and normalized:\nKeyword variants are matched against the predefined keyword list.\nOnly exact or partial matches are retained.\nValid causal pairs are stored with frequency counts.\nFor instance, if “crop damage” is detected but only “crop” is in the keyword list, the system maps the phrase to “crop” to preserve consistency.\nGraph Construction The extracted causal pairs are added to a Directed Acyclic Graph (DAG) structure:\nEach node represents a concept (e.g., “weather” or “fatigue”).\nEach edge represents a directional cause-effect relation (e.g., “weather” → “delay”).\nEdges can carry weights indicating how often a particular causal link appeared across different reports.\nExemple of usage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 if __name__ == \u0026#34;__main__\u0026#34;: # Aviation safety keywords keywords = [ \u0026#39;crack\u0026#39;, \u0026#39;passenger\u0026#39;, \u0026#39;turbulence\u0026#39;, \u0026#39;attendant\u0026#39;, \u0026#39;seat\u0026#39;, \u0026#39;taxi\u0026#39;, \u0026#39;crew\u0026#39;, \u0026#39;fuel\u0026#39;, \u0026#39;power\u0026#39;, \u0026#39;engine\u0026#39;, \u0026#39;fire\u0026#39;, \u0026#39;weather\u0026#39;, \u0026#39;runway\u0026#39;, \u0026#39;controller\u0026#39;, \u0026#39;traffic\u0026#39;, \u0026#39;captain\u0026#39;, \u0026#39;pushback\u0026#39;, \u0026#39;nose\u0026#39;, \u0026#39;door\u0026#39;, \u0026#39;gear\u0026#39;, \u0026#39;landing\u0026#39;, \u0026#39;blade\u0026#39;, \u0026#39;takeoff\u0026#39;, \u0026#39;pilot\u0026#39;, \u0026#39;aircraft\u0026#39;, \u0026#39;collision\u0026#39;, \u0026#39;landing\u0026#39; ] # Sample aviation incident reports (tokenized) documents = [ \u0026#34;The engine failure caused loss of power during takeoff\u0026#34;.split(), \u0026#34;Pilot fatigue contributed to runway excursion\u0026#34;.split(), \u0026#34;Severe weather led to turbulence and passenger injuries\u0026#34;.split(), \u0026#34;Crack in the blade resulted in engine fire\u0026#34;.split(), \u0026#34;Controller poor communication cause pilot bad judment leading to collision\u0026#34;.split(), \u0026#34;Improper pushback procedure caused nose gear damage\u0026#34;.split(), \u0026#34;Fuel contamination led to engine power loss and emergency landing\u0026#34;.split(), \u0026#34;Controller error resulted in traffic conflict on taxiway\u0026#34;.split(), \u0026#34;Cabin door issue during flight led to emergency dlanding\u0026#34;.split(), \u0026#34;Poor weather conditions contributed to collision with terrain\u0026#34;.split() ] # Step 1: Extract causal relations from aviation reports print(\u0026#34;Extracting causal relations from aviation reports...\u0026#34;) causal_relations = extract_causal_relations(documents, keywords) print(\u0026#34;Discovered aviation safety relations:\u0026#34;, dict(causal_relations)) # Step 2: Build the aviation safety network print(\u0026#34;Building aviation safety network...\u0026#34;) safety_net_graph = build_custom_dag(causal_relations) # Step 3: Extract all causal paths in aviation context paths = safety_net_graph.get_paths() print(\u0026#34;\\nAviation Safety Causal Paths:\u0026#34;) for i, path in enumerate(paths, 1): print(f\u0026#34;{i}. {\u0026#39; → \u0026#39;.join(path)}\u0026#34;) # Step 4: Visualize aviation safety network print(\u0026#34;Generating aviation safety visualization...\u0026#34;) safety_net_graph.visualize_graphviz(\u0026#39;aviation_safety_before\u0026#39;) print(\u0026#34;\\nAviation Safety Graph Structure:\u0026#34;) print(safety_net_graph) # Step 5: Insert aviation-specific nodes safety_net_graph.insert_node(child=\u0026#39;weather\u0026#39;, cause_name=\u0026#39;meteorological\u0026#39;) print(\u0026#34;\\nAfter inserting \u0026#39;meteorological\u0026#39; as parent of \u0026#39;weather\u0026#39;:\u0026#34;) print(safety_net_graph) safety_net_graph.insert_node(parent=\u0026#39;pilot\u0026#39;, cause_name=\u0026#39;accident\u0026#39;) print(\u0026#34;\\nAfter inserting \u0026#39;accident\u0026#39; as child of \u0026#39;pilot\u0026#39;:\u0026#34;) print(safety_net_graph) # Adding intermediate factor between pilot and accident safety_net_graph.insert_node(parent=\u0026#39;pilot\u0026#39;, child=\u0026#39;accident\u0026#39;, cause_name=\u0026#39;judgment\u0026#39;) print(\u0026#34;\\nAfter inserting \u0026#39;judgment\u0026#39; between \u0026#39;pilot\u0026#39; and \u0026#39;accident\u0026#39;:\u0026#34;) print(safety_net_graph) # Step 6: Remove a node safety_net_graph.remove_node(node=\u0026#39;judgment\u0026#39;) print(\u0026#34;\\nAfter deleting \u0026#39;judgment\u0026#39;:\u0026#34;) print(safety_net_graph) # Additional aviation-specific analysis print(\u0026#34;\\nRoot causes in aviation system:\u0026#34;) print(safety_net_graph.get_roots()) print(\u0026#34;\\nFinal outcomes in aviation incidents:\u0026#34;) print(safety_net_graph.get_leaves()) safety_net_graph.visualize_graphviz(\u0026#39;aviation_safety_after\u0026#39;) Figure 2: Directed Acyclic Graph built before insertion/deletion operations\nFigure 3: Directed Acyclic Graph built after insertion/deletion operations\nAdvantages of the LLM Approach Traditional rule-based extraction methods rely on fixed rules (e.g., detecting words like “because,” “due to”) and struggle with complex sentence structures, especially when:\nMultiple causes or effects are described,\nRelationships span multiple clauses,\nNegations or uncertainties are involved.\nBy contrast, the LLM can interpret nuanced linguistic patterns and deliver structured output from natural sentences without requiring hard-coded rules. It generalizes across variations in grammar, phrasing, and report writing styles.\n","date":"2025-06-11T00:00:00Z","image":"https://sarahfrodrigues.github.io/my-new-site/p/assignment-4-developing-an-application-with-the-aid-of-llm/cover4_hu_bb4a8dc1e0adff79.jpg","permalink":"https://sarahfrodrigues.github.io/my-new-site/p/assignment-4-developing-an-application-with-the-aid-of-llm/","title":"Assignment 4 - Developing an Application with the Aid of LLM"},{"content":"AI Model API Comparison Project Project Overview This project compares the performance and features of online and local AI models for code generation tasks. It provides:\nA FastAPI interface for model queries Performance benchmarking capabilities Side-by-side feature comparison Online vs Local Models When selecting between online and local AI models, developers should consider trade-offs in accessibility, performance, privacy, and scalability:\nOnline Models Online models, such as Hugging Face-hosted Qwen2.5-32B, are accessed via API over the internet. These models typically offer:\nAccess to powerful infrastructure and large-scale models (e.g., 32B parameters) Minimal local setup required Automatic updates and optimizations handled by the provider Dependence on a stable internet connection and API key Potential latency due to network requests Usage limits or costs depending on the provider\u0026rsquo;s pricing plan Local Models Local models like CodeLLaMA or Qwen2.5-3B run entirely on a user\u0026rsquo;s machine. They offer:\nFull offline capability and independence from external services Greater control over execution and data privacy No usage quotas or API restrictions Hardware requirements depending on model size Slightly more effort for initial setup and configuration Online Model Setup 1. API Provider Selection Hugging Face was selected as the API provider because:\nOffers state-of-the-art open models Provides generous free tier for experimentation Has excellent documentation and community support Supports the Qwen series of coding-specialized models 2. Obtaining API Key Go to Hugging Face website Create an account or log in Navigate to Access Tokens Create a new token with write permission Securely store your key in a .env file: 1 HF_API_KEY=your_api_key_here 3. Model Selection Selected model: Qwen/Qwen2.5-Coder-32B-Instruct\nKey features:\n32 billion parameter model Specialized for coding tasks Supports 32k token context window Strong Python code generation capabilities 4. Python Environment Setup Required packages:\n1 pip install fastapi requests python-dotenv uvicorn Local Model Development 1. Ollama Installation Download Ollama from ollama.ai Install following platform-specific instructions 1 ollama run model-name 2. Model Selection Model Name Size Strengths Installation Command codellama:7b 7B General-purpose code gen ollama pull codellama:7b qwen2.5-coder:3b 3B Better complex code reasoning ollama pull qwen2.5-coder 3. Python Environment Additional required package:\n1 pip install ollama Model Comparison The model comparison was implemented in Python to evaluate the behavior of different models under the same conditions.\nThe following code snippet demonstrates the structure used for this comparison:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 from fastapi import FastAPI, Query from fastapi.responses import JSONResponse, PlainTextResponse, HTMLResponse from pydantic import BaseModel import requests import os from dotenv import load_dotenv # Load .env file load_dotenv() # Constants HF_API_KEY = os.getenv(\u0026#34;HF_API_KEY\u0026#34;) MODEL_NAME = \u0026#34;Qwen/Qwen2.5-Coder-32B-Instruct\u0026#34; API_URL = f\u0026#34;https://api-inference.huggingface.co/models/{MODEL_NAME}\u0026#34; # Initialize FastAPI app app = FastAPI() class QueryRequest(BaseModel): \u0026#34;\u0026#34;\u0026#34; Request model for text generation endpoint. Attributes: prompt: The input text prompt for the model. max_tokens: Maximum number of tokens to generate (default: 512). \u0026#34;\u0026#34;\u0026#34; prompt: str max_tokens: int = 512 def query_model(prompt: str, max_tokens: int = 512) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Query the Hugging Face model API with the given prompt. Args: prompt: Input text for the model. max_tokens: Maximum number of tokens to generate. Returns: Generated text from the model. Raises: requests.exceptions.RequestException: If the API request fails. \u0026#34;\u0026#34;\u0026#34; headers = {\u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {HF_API_KEY}\u0026#34;} payload = { \u0026#34;inputs\u0026#34;: prompt, \u0026#34;parameters\u0026#34;: {\u0026#34;max_new_tokens\u0026#34;: max_tokens} } response = requests.post(API_URL, headers=headers, json=payload) response.raise_for_status() return response.json()[0].get(\u0026#34;generated_text\u0026#34;, \u0026#34;\u0026#34;) def format_response(prompt: str, response: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Format the response with clear prompt and response sections. Args: prompt: The original user prompt response: The generated response from the model Returns: Formatted string with prompt and response sections \u0026#34;\u0026#34;\u0026#34; # Remove the original prompt if it\u0026#39;s duplicated in the response cleaned_response = response.replace(prompt, \u0026#34;\u0026#34;, 1).strip() return f\u0026#34;prompt:\\n{prompt}\\n\\nresponse:\\n{cleaned_response}\u0026#34; @app.post(\u0026#34;/generate\u0026#34;) async def generate_text( request: QueryRequest, format: str = Query( \u0026#34;text\u0026#34;, description=\u0026#34;Output format\u0026#34;, enum=[\u0026#34;text\u0026#34;, \u0026#34;json\u0026#34;, \u0026#34;html\u0026#34;] ) ): \u0026#34;\u0026#34;\u0026#34; Generate text using the Hugging Face model. Args: request: QueryRequest containing prompt and max_tokens. format: Output format (text, json, or html). Returns: Response containing generated text in the requested format. \u0026#34;\u0026#34;\u0026#34; try: raw_response = query_model(request.prompt, request.max_tokens) formatted_response = format_response(request.prompt, raw_response) if format == \u0026#34;json\u0026#34;: return JSONResponse(content={ \u0026#34;prompt\u0026#34;: request.prompt, \u0026#34;response\u0026#34;: raw_response.replace(request.prompt, \u0026#34;\u0026#34;, 1).strip() }) elif format == \u0026#34;html\u0026#34;: return HTMLResponse( content=f\u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h3\u0026gt;Prompt:\u0026lt;/h3\u0026gt; \u0026lt;pre\u0026gt;{request.prompt}\u0026lt;/pre\u0026gt; \u0026lt;h3\u0026gt;Response:\u0026lt;/h3\u0026gt; \u0026lt;pre\u0026gt;{raw_response.replace(request.prompt, \u0026#34;\u0026#34;, 1).strip()}\u0026lt;/pre\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; ) else: return PlainTextResponse(content=formatted_response) except requests.exceptions.RequestException as e: return PlainTextResponse( content=f\u0026#34;Error: {str(e)}\u0026#34;, status_code=500 ) if __name__ == \u0026#34;__main__\u0026#34;: # Test the model with proper formatting sample_query = \u0026#34;Write a quick sort algorithm in Python.\u0026#34; response = query_model(sample_query) print(format_response(sample_query, response)) Benchmark Results === Model Comparison ===\nModel: online_qwen Type: Online Time: 0.54s Response length: 1938 chars\nResponse excerpt: Write a quick sort algorithm in Python. Certainly! Quick sort is a popular and efficient sorting algorithm that uses a divide-and-conquer approach to sort elements. Here\u0026rsquo;s a simple implementation of the quick sort algorithm in Python:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def quick_sort(arr): # Base case: if the array is empty or has one element, it\u0026#39;s already sorted if len(arr) \u0026lt;= 1: return arr else: # Choose a pivot element from the array pivot = arr[len(arr) // 2] # Partition the array into three parts: # - elements less than the pivot # - elements equal to the pivot # - elements greater than the pivot less = [x for x in arr if x \u0026lt; pivot] equal = [x for x in arr if x == pivot] greater = [x for x in arr if x \u0026gt; pivot] # Recursively apply quick_sort to the \u0026#39;less\u0026#39; and \u0026#39;greater\u0026#39; subarrays return quick_sort(less) + equal + quick_sort(greater) # Example usage: arr = [3, 6, 8, 10, 1, 2, 1] sorted_arr = quick_sort(arr) print(\u0026#34;Sorted array:\u0026#34;, sorted_arr) Explanation:\nBase Case: If the array has one or zero elements, it is already sorted, so we return it as is. Pivot Selection: We choose the middle element of the array as the pivot. This is a simple strategy, but there are other strategies for choosing a pivot that can improve performance in certain cases. Partitioning: We partition the array into three lists: less: Elements less than the pivot. equal: Elements equal to the pivot. greater: Elements greater than the pivot. Recursive Sorting: We recursively sort the less and greater lists and concatenate them with the equal list to get the sorted array. This implementation is straightforward and works well for educational purposes. However, in practice, optimizations such as choosing a better pivot and using in-place partitioning can improve performance.\nModel: local_codellama Type: Local Time: 13.07s Response length: 756 chars\nResponse excerpt:\nHere is an example of a quicksort algorithm in Python:\n1 2 3 4 5 6 7 def quicksort(arr): if len(arr) \u0026lt;= 1: return arr pivot = arr[0] less = [x for x in arr[1:] if x \u0026lt;= pivot] greater = [x for x in arr[1:] if x \u0026gt; pivot] return quicksort(less) + [pivot] + quicksort(greater) This algorithm works by selecting a pivot element from the array, and then partitioning the array into two subarrays: one containing elements less than the pivot, and one containing elements greater than the pivot. It then recursively sorts the subarrays and combines them to produce the sorted array.\nHere is an example of how you can use this algorithm to sort a list of numbers:\n1 2 3 arr = [3, 2, 1, 5, 4] print(quicksort(arr)) # Output: [1, 2, 3, 4, 5] Model: local_qwen Type: Local Time: 9.78s Response length: 1340 chars\nResponse excerpt: Certainly! Here\u0026rsquo;s a simple implementation of the QuickSort algorithm in Python:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def quick_sort(arr): if len(arr) \u0026lt;= 1: return arr else: pivot = arr[len(arr) // 2] left = [x for x in arr if x \u0026lt; pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x \u0026gt; pivot] return quick_sort(left) + middle + quick_sort(right) # Example usage: example_array = [3, 6, 8, 10, 1, 2, 1] sorted_array = quick_sort(example_array) print(sorted_array) Explanation:\nBase Case: If the array has 0 or 1 element, it is already sorted, so we return it as is. Pivot Selection: We choose a pivot element from the array. In this implementation, we use the middle element of the array as the pivot. Partitioning: left: A list containing all elements less than the pivot. middle: A list containing all elements equal to the pivot. right: A list containing all elements greater than the pivot. Recursive Sorting: We recursively apply the quicksort algorithm to the left and right lists and concatenate the results with the middle list. This implementation of QuickSort is in-place, meaning it doesn\u0026rsquo;t require extra space for another array. However, the space complexity is O(log n) due to the recursion stack.\n1. Performance Evaluation Response Time: Measures the speed of each model in generating output. Response Length: Captures the character count of the output. Quality Assessment: Subjective evaluation based on accuracy, coherence, and code correctness. 2. Feature Comparison Feature Online Qwen2.5-32B Local CodeLLaMA Local Qwen2.5 Model Parameters 32B 7B 3B Model Size Cloud-hosted 3.8 GB 1.9 GB Requires Internet Yes No No API Key Needed Yes No No Context Window 32K tokens 16K tokens 32K tokens Streaming Support No Yes Yes Setup Complexity Medium Low Low Recommended Use Case Production Development Development Conclusion This project demonstrated how developers can effectively integrate both online and local large language models (LLMs) into their development environments. By leveraging cloud-based APIs like Hugging Face and local model runtimes such as Ollama, we explored a flexible approach to AI-assisted coding workflows.\nThe comparison revealed key trade-offs: online models provide access to larger, production-ready models with minimal setup, while local models offer privacy, offline capability, and greater control. Both approaches have their strengths depending on the use case—whether it\u0026rsquo;s rapid prototyping, secure offline development, or cost-effective experimentation.\nUltimately, this setup empowers developers to:\nEvaluate different AI model configurations in a consistent testing framework Balance performance, resource usage, and deployment complexity Seamlessly switch between remote and local inference based on project requirements As LLMs continue to evolve, understanding how to deploy and integrate them efficiently will be an essential skill for modern software development.\n","date":"2025-06-09T00:00:00Z","image":"https://sarahfrodrigues.github.io/my-new-site/p/assignment-3-deployment-and-integration-of-llm/cover3_hu_28abe42c3a774817.jpg","permalink":"https://sarahfrodrigues.github.io/my-new-site/p/assignment-3-deployment-and-integration-of-llm/","title":"Assignment 3 - Deployment and Integration of LLM"},{"content":"Table of Contents Introduction 1.1. Markdown Documentation 1.2. Compiled vs Interpreted Language System Configuration Implementation Details 3.1. C Language Implementation 3.2. Python Language Implementation Algorithm Verification Performance Analysis Conclusion References Appendix Introduction This project implements matrix multiplication in both a compiled language (C) and an interpreted language (Python) to explore their differences in performance and development workflows, while use markdown documentation to ensures code is understandable, maintainable, and reproducible.\n\u0026ldquo;Documentation is a love letter that you write to your future self.\u0026rdquo; – Damian Conway\nThis project aims to:\nFamiliarize with Unix/Linux command-line operations. Develop proficiency in Markdown for technical documentation. Implement matrix multiplication using both a compiled language (C) and an interpreted language (Python). Validate the correctness of the algorithm and analyze performance differences. Get me a good grade in the software course Markdown Documentation Markdown is a lightweight markup language that simplifies formatting for text documents. It was created by John Gruber in 2004 and is widely used for technical documentation, blogging, and readme files.\nIt allows users to write in plain text while incorporating headings, lists, links, and code blocks with minimal syntax.\nExample of a code block in Markdown:\n1 2 3 # This is a header - List item **Bold text** To learn more about Markdown, visit Markdown Guide.\nCompiled vs Interpreted Language Every program consists of a set of instructions that need to be converted into machine code for execution. This conversion is handled by compilers and interpreters.\nCompilation languages like Java are generally faster in genetic algorithms, but other languages, including interpreted ones, can hold their ground against them. Louden, K., \u0026amp; Mak, R. (2014). Compilers and Interpreters. , 68: 1-37. https://doi.org/10.1201/B16812-77.\nCompiled Languages A compiled language translates the entire source code into machine code before execution. This results in faster performance since the program runs directly on the processor without needing real-time translation.\nInterpreted Languages An interpreted language translates and executes code line by line during runtime, providing more flexibility but often at the cost of speed.\nComparison Feature Compiled Language Interpreted Language Execution Process Source code → Machine code → Execution Source code → Execution (direct interpretation) Speed Faster Slower Performance High Lower Compilation Required? Yes, before execution No, executed line by line Examples C, C++, Java Python, JavaScript, Perl Figure 1: Compiled language vs interpreted language1\nSystem Configuration Accurate performance evaluation requires detailed documentation of the experimental environment. The table below specifies the hardware and software configuration used for this study, collected entirely through Linux command-line operations.\nComponent Details CPU Model 13th Gen Intel(R) Core(TM) i7-13650HX Architecture x86_64 Memory Size 7.6Gi total, 594Mi used, 7.0Gi free, 3.1Mi shared, 244Mi buff/cache, 7.0Gi available Swap 2.0Gi total, 0B used, 2.0Gi free Operating System Version Linux ac4raj7 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 GNU/Linux Compiler Version gcc (Debian 12.2.0-14) 12.2.0 Copyright (C) 2022 Free Software Foundation, Inc. Python Version Python 3.9.12 Implementation Details C Language Implementation Source Code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include \u0026lt;stdlib.h\u0026gt; // Standard library #include \u0026lt;time.h\u0026gt; // Time library double** create_matrix(int size) { // Function to create a matrix double** matrix = (double**)malloc(size * sizeof(double*)); // Allocate memory for rows for(int i=0; i\u0026lt;size; i++) { // Loop over rows of the matrix matrix[i] = (double*)aligned_alloc(64, size * sizeof(double)); // Allocate memory for columns with alignment } return matrix; } void matrix_multiply(double** A, double** B, double** C, int size) { // Function to multiply two matrices #pragma omp parallel for collapse(2) // Parallelize the nested loops for(int i=0; i\u0026lt;size; i++) { // Loop over rows of matrix A for(int j=0; j\u0026lt;size; j++) { // Loop over columns of matrix B double sum = 0.0; // Initialize sum to 0 #pragma omp simd reduction(+:sum) // SIMDize the loop and use reduction for sum for(int k=0; k\u0026lt;size; k++) { // Loop over columns of matrix A and rows of matrix B sum += A[i][k] * B[k][j]; // Multiply corresponding elements and add to sum } C[i][j] = sum; // Assign sum to the corresponding element of matrix C } } } Compilation Command: 1 gcc matrix_multiply.c -o matrix_multiply # Compile Execution Command: 1 ./matrix_multiply # Run Python Language Implementation Source Code: 1 2 3 4 # Python implementation of matrix multiplication def multiply_matrices(A, B): result = [[sum(a * b for a, b in zip(A_row, B_col)) for B_col in zip(*B)] for A_row in A] return result Execution Command: 1 python3 matrix_multiplication.py Algorithm Verification Validation methodology: Unit Testing (PyTest/Catch2) Cross-Language Consistency Checks Edge Case Analysis Test Case C Result Python Result Validation Identity Matrix Pass Pass ✅ Zero Matrix Pass Pass ✅ Non-square (3×4 × 4×2) Pass Pass ✅ Floating Point Precision 1e-9 tol 1e-9 tol ✅ Performance Analysis【bonus】 To asses the performance of the both languages the following codes were perfomed evaluating the average execution time for different matrix sizes\nImplementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #include \u0026lt;stdio.h\u0026gt; // Standard input/output library #include \u0026lt;stdlib.h\u0026gt; // Standard library #include \u0026lt;time.h\u0026gt; // Time library void matrix_multiply(int **A, int **B, int **result, int size) { for (int i = 0; i \u0026lt; size; i++) { for (int j = 0; j \u0026lt; size; j++) { result[i][j] = 0; for (int k = 0; k \u0026lt; size; k++) result[i][j] += A[i][k] * B[k][j]; } } } // Function to allocate memory for a matrix int** allocate_matrix(int size) { int **matrix = (int **)malloc(size * sizeof(int *)); for (int i = 0; i \u0026lt; size; i++) matrix[i] = (int *)malloc(size * sizeof(int)); return matrix; } // Function to free allocated matrix memory void free_matrix(int **matrix, int size) { for (int i = 0; i \u0026lt; size; i++) free(matrix[i]); free(matrix); } // Function to initialize matrix with random values void initialize_matrix(int **matrix, int size) { for (int i = 0; i \u0026lt; size; i++) for (int j = 0; j \u0026lt; size; j++) matrix[i][j] = rand() % 10; // Values between 0-9 } int main() { srand(time(NULL)); // Seed for random numbers int sizes[] = {5, 10, 50, 100, 500, 1000}; // Different matrix sizes to test int num_sizes = sizeof(sizes) / sizeof(sizes[0]); int num_runs = 10; // Number of runs for averaging for (int s = 0; s \u0026lt; num_sizes; s++) { int size = sizes[s]; printf(\u0026#34;\\nTesting matrix size: %dx%d\\n\u0026#34;, size, size); // Allocate memory for matrices int **A = allocate_matrix(size); int **B = allocate_matrix(size); int **result = allocate_matrix(size); // Initialize matrices with random values initialize_matrix(A, size); initialize_matrix(B, size); double total_time = 0.0; // Run multiple times to get an average execution time for (int run = 0; run \u0026lt; num_runs; run++) { clock_t start = clock(); matrix_multiply(A, B, result, size); clock_t end = clock(); total_time += (double)(end - start) / CLOCKS_PER_SEC; } printf(\u0026#34;Average execution time over %d runs: %f seconds\\n\u0026#34;, num_runs, total_time / num_runs); // Free allocated memory free_matrix(A, size); free_matrix(B, size); free_matrix(result, size); } return 0; } Python\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 import time # Fixed time import import random # Fixed random import import matplotlib.pyplot as plt # Fixed matplotlib import def multiply_matrices(A, B): \u0026#34;\u0026#34;\u0026#34; Multiply two matrices A and B using nested list comprehensions. Args: A (list of lists): First matrix B (list of lists): Second matrix Returns: list of lists: Resultant matrix after multiplication \u0026#34;\u0026#34;\u0026#34; # Transpose B once to access columns efficiently B_transposed = list(zip(*B)) # Perform matrix multiplication using pre-transposed B return [ [ sum(a * b for a, b in zip(A_row, B_col)) for B_col in B_transposed ] for A_row in A ] def generate_matrix(size): \u0026#34;\u0026#34;\u0026#34; Generate a square matrix of given size with random integers between 0 and 10. Args: size (int): Size of the square matrix Returns: list of lists: Generated matrix \u0026#34;\u0026#34;\u0026#34; return [[random.randint(0, 10) for _ in range(size)] for _ in range(size)] # Matrix sizes to test (consider reducing max size for faster execution) sizes = [5, 10, 50, 100, 500, 1000] num_runs = 10 # Number of runs per size (reduce for larger matrices) results = {} # Performance testing loop for size in sizes: total_time = 0.0 print(f\u0026#34;Testing matrix size: {size}x{size}\u0026#34;) # Warm-up loop to account for cold-start effects A = generate_matrix(size) B = generate_matrix(size) multiply_matrices(A, B) for _ in range(num_runs): # Generate fresh matrices for each run A = generate_matrix(size) B = generate_matrix(size) # Time only the multiplication start_time = time.perf_counter() multiply_matrices(A, B) end_time = time.perf_counter() total_time += (end_time - start_time) avg_time = total_time / num_runs results[size] = avg_time print(f\u0026#34;Average execution time: {avg_time:.6f} seconds\u0026#34;) # Save results to file with open(\u0026#34;python_performance_data.txt\u0026#34;, \u0026#34;w\u0026#34;) as file: for size, avg_time in results.items(): file.write(f\u0026#34;{size} {avg_time}\\n\u0026#34;) # Plotting results plt.figure(figsize=(10, 6)) # Convert sizes to sorted list for proper plotting x = sorted(results.keys()) y = [results[size] for size in x] plt.plot(x, y, marker=\u0026#39;o\u0026#39;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;r\u0026#39;, label=\u0026#34;Python Matrix Multiplication Time\u0026#34;) plt.xscale(\u0026#39;log\u0026#39;) # Logarithmic scale for better visualization plt.yscale(\u0026#39;log\u0026#39;) plt.xlabel(\u0026#34;Matrix Size (NxN)\u0026#34;) plt.ylabel(\u0026#34;Execution Time (seconds)\u0026#34;) plt.title(\u0026#34;Performance of Python Matrix Multiplication (Optimized)\u0026#34;) plt.legend() plt.grid(True, which=\u0026#34;both\u0026#34;, ls=\u0026#34;--\u0026#34;) plt.savefig(\u0026#34;performance_plot.png\u0026#34;) # Save plot to file plt.show() Execution Times Matrix Size C Time (s) Python Time (s) Speed Ratio 5x5 0.000003 0.000012 4.0x 10x10 0.000007 0.000062 8.9x 50x50 0.00042 0.005062 12.1x 100x100 0.0030 0.037920 12.6x 500x500 0.375 4.542135 12.1x 1000x1000 2.824 36.586664 13.0x Analysis The C implementation demonstrates significantly better performance, as expected for a compiled language. Several factors contribute to this difference. First, C uses static typing, which eliminates the overhead of dynamic type checking present in Python. Additionally, C provides direct memory access, allowing it to manage memory allocation without the automatic memory management and garbage collection overhead seen in Python. Another major factor is Python’s Global Interpreter Lock (GIL), which restricts its ability to fully utilize multi-threading, particularly affecting computationally intensive tasks. Furthermore, Python relies on higher-level abstractions for its built-in data structures, such as lists, which are not optimized for numerical computations. While libraries like NumPy improve performance, pure Python remains inefficient for such operations.\nConclusion This study highlights the significant performance differences between compiled (C) and interpreted (Python) languages in numerical computations. The key takeaways from this analysis are that we successfully implemented matrix multiplication in both C and Python, enabling a direct performance comparison. Markdown was used to document and present results. While Python is convenient for development, it suffers from runtime overhead.\nReferences ","date":"2025-04-23T00:00:00Z","image":"https://sarahfrodrigues.github.io/my-new-site/p/assignment-1/cover1_hu_d02e8bbd000259eb.jpg","permalink":"https://sarahfrodrigues.github.io/my-new-site/p/assignment-1/","title":"Assignment 1 - Command Line \u0026 Documentation"},{"content":"Introduction to Static Sites Static websites consist of fixed content delivered to users exactly as stored, unlike dynamic websites that generate content on-the-fly. Benefits include:\nPerformance: Faster load times with pre-built pages Security: Reduced attack surface with no server-side processing Scalability: Easier to handle traffic spikes Cost-effectiveness: Lower hosting requirements Key Advantages of Static Sites Feature Benefit ⚡ Performance Pre-built pages load instantly with minimal server processing 🔒 Security No database or server-side scripts reduces attack vectors 📈 Scalability Easily handles traffic spikes with simple file serving 💰 Cost Can be hosted on low-cost or free platforms like GitHub Pages 🔄 Versioning Content is file-based, making it ideal for Git workflows Note: While static sites are excellent for content that doesn\u0026rsquo;t change frequently, they can be enhanced with JavaScript for dynamic functionality when needed.\nWhy Hugo? Hugo is a modern static site generator written in Go, offering:\nBlazing fast build times (often \u0026lt;1ms per page) Rich theming system with hundreds of community themes Flexible content organization with sections and taxonomies Built-in development server with live reload Markdown-based content with front matter support Installation Guide Windows Installation Download the Extended version:\nVisit Hugo Releases and look for the latest version Under \u0026ldquo;Assets\u0026rdquo;, download hugo_extended_*.msi Run the installer:\nDouble-click the downloaded .msi file Follow the installation wizard (recommend keeping default options) Verify installation:\n1 hugo version 1 2 3 4 5 6 7 8 You should see output like hugo v0.xxx windows/amd64 Extended confirming the Extended build is installed. ## Project Setup 1. **Initialize Your Site** ```bash hugo new site my-blog cd my-blog git init Once its done a directory structure is created: my-blog/ ├── archetypes/ ├── assets/ ├── content/ ├── data/ ├── layouts/ ├── static/ ├── themes/ └── config.yaml\nAdd a Theme Many themes can be choosen in https://themes.gohugo.io/ that can be downloaded. For this project i choosed to use the stack theme 1 git submodule add https://github.com/CaiJimmy/hugo-theme-stack themes/stack Add to config.toml:\n1 theme = \u0026#34;stack\u0026#34; Configure Basic Settings Edit config.toml with essential parameters: 1 2 3 baseURL = \u0026#34;https://yourusername.github.io/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;My Personal Blog\u0026#34; Content Creation Creating Posts Generate a new post:\n1 hugo new posts/assignment-1.md Example post structure:\n1 2 3 4 5 6 7 8 9 10 --- title: \u0026#34;Assignment 1: Static Site Setup\u0026#34; date: 2025-04-23T15:30:00Z draft: false tags: [\u0026#39;Hugo\u0026#39;, \u0026#39;Static Sites\u0026#39;] categories: [\u0026#39;Assignments\u0026#39;] description: \u0026#34;Initial setup of my static blog using Hugo\u0026#34; --- ## Introduction Content goes here... Additional Pages Create an about page:\n1 hugo new about.md GitHub Pages Deployment Create a GitHub repository named yourusername.github.io\nAdd remote and push:\n1 2 git remote add origin git@github.com:yourusername/yourusername.github.io.git git push -u origin main Enable GitHub Pages in repository settings (use gh-pages branch) Version Control To do the version control, I used Git to track and manage the progress of my static blog website. This ensured that every change was recorded, recoverable, and well-documented throughout the development process.\n1 2 3 git add . git commit -m \u0026#34;Add Stack theme and configure base settings\u0026#34; git push origin main This were the key commits and changes made during the development of the static blog website.\n✅ Initial Commit 🧱 Establishes the foundational structure of the project. 🛠️ Includes the basic Hugo setup and essential configuration files. 🌱 Serves as a clean starting point for all future development. 📝 Assignment 1 (Completed) ✍️ Adds specific content and analysis for Assignment 1. 📄 Includes the Markdown post file and related media assets. 🔄 Enables easy rollback or isolation of this content if needed. 🚧 Assignment 2 (In Progress) 📌 Creates the draft structure for Assignment 2 post. 🗂️ Organizes Markdown content and placeholders for upcoming edits. 🎨 Sidebar Configuration 🧩 Customizes the sidebar layout and user interface. 🐝 Adds emoji and inspirational quote as subtitle. 🧭 Modifies navigation and theme layout for a more personalized touch. 🔜 Assignments 3 \u0026amp; 4 (Draft Setup) 🗓️ Prepares draft pages for future assignments. 📚 Establishes a consistent structure and naming convention. 📖 Documentation Draft 📝 Begins explanation of the project setup and configuration process. 📌 Includes markdown usage, theme setup, and development tips. 👤 About Page Configuration 📄 Creates a static \u0026ldquo;About Me\u0026rdquo; page. 🧬 Adds biography and academic background. 🌟 Important page that might be updated frequently as the site evolves. ","date":"2025-04-23T00:00:00Z","image":"https://sarahfrodrigues.github.io/my-new-site/p/assignment-2/cover2_hu_d365a67c22ee590c.jpg","permalink":"https://sarahfrodrigues.github.io/my-new-site/p/assignment-2/","title":"Assignment 2 - Static Personal Blog Website"}]